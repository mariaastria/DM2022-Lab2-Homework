{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Student Information\nName: Maria Astria Viandi æž—èŠ³èŠ³\n\nStudent ID: 108006225\n\nGitHub ID: mariaastria\n\nKaggle name: Maria Astria\n\nKaggle private scoreboard snapshot: \n\n[Snapshot](img/pic0.png)","metadata":{}},{"cell_type":"markdown","source":"### Instructions","metadata":{}},{"cell_type":"markdown","source":"1. First: __This part is worth 30% of your grade.__ Do the **take home** exercises in the [DM2022-Lab2-master Repo](https://github.com/keziatamus/DM2022-Lab2-Master). You may need to copy some cells from the Lab notebook to this notebook. \n\n\n2. Second: __This part is worth 30% of your grade.__ Participate in the in-class [Kaggle Competition](https://www.kaggle.com/competitions/dm2022-isa5810-lab2-homework) regarding Emotion Recognition on Twitter by this link https://www.kaggle.com/t/2b0d14a829f340bc88d2660dc602d4bd. The scoring will be given according to your place in the Private Leaderboard ranking: \n    - **Bottom 40%**: Get 20% of the 30% available for this section.\n\n    - **Top 41% - 100%**: Get (60-x)/6 + 20 points, where x is your ranking in the leaderboard (ie. If you rank 3rd your score will be (60-3)/6 + 20 = 29.5% out of 30%)   \n    Submit your last submission __BEFORE the deadline (Nov. 22th 11:59 pm, Tuesday)_. Make sure to take a screenshot of your position at the end of the competition and store it as '''pic0.png''' under the **img** folder of this repository and rerun the cell **Student Information**.\n    \n\n3. Third: __This part is worth 30% of your grade.__ A report of your work developping the model for the competition (You can use code and comment it). This report should include what your preprocessing steps, the feature engineering steps and an explanation of your model. You can also mention different things you tried and insights you gained. \n\n\n4. Fourth: __This part is worth 10% of your grade.__ It's hard for us to follow if your code is messy :'(, so please **tidy up your notebook** and **add minimal comments where needed**.\n\n\nUpload your files to your repository then submit the link to it on the corresponding e-learn assignment.\n\nMake sure to commit and save your changes to your repository __BEFORE the deadline (Nov. 25th 11:59 pm, Friday)__. ","metadata":{}},{"cell_type":"markdown","source":"# Data Mining Lab 2\nIn this lab session we will focus on the use of Neural Word Embeddings ","metadata":{}},{"cell_type":"markdown","source":"## Table of Contents\n1. Data preparation\n2. Feature engineering\n3. Model\n4. Results evaluation\n5. Other things you could try\n6. Deep Learning\n7. Word to Vector\n8. Clustering\n9. High-dimension Visualization\n\n","metadata":{"id":"ledffNYYYTpX"}},{"cell_type":"markdown","source":"## Necessary Library Requirements:\n\n#### Same as Lab1:\n- [Jupyter](http://jupyter.org/) (Strongly recommended but not required)\n    - Install via `pip3 install jupyter` and use `jupyter notebook` in terminal to run\n- [Scikit Learn](http://scikit-learn.org/stable/index.html)\n    - Install via `pip3 sklearn` from a terminal\n- [Pandas](http://pandas.pydata.org/)\n    - Install via `pip3 install pandas` from a terminal\n- [Numpy](http://www.numpy.org/)\n    - Install via `pip3 install numpy` from a terminal\n- [Matplotlib](https://matplotlib.org/)\n    - Install via `pip3 maplotlib` from a terminal\n- [Plotly](https://plot.ly/)\n    - Install via `pip3 install plotly` from a terminal\n- [Seaborn](https://seaborn.pydata.org/)\n    - Install and signup for `seaborn`\n- [NLTK](http://www.nltk.org/)\n    - Install via `pip3 install nltk` from a terminal\n\n#### New Libraries to intsall:\n- [Gensim](https://pypi.org/project/gensim/)\n    - Install via `pip3 install gensim`\n\n- [tensorflow](https://www.tensorflow.org/)\n    - Install via `pip3 install tensorflow`\n    - Also install `pip3 install tensorflow-hub`\n\n- [Keras](https://keras.io/)\n    - Install via `pip3 install keras`","metadata":{}},{"cell_type":"markdown","source":"---\n# Introduction","metadata":{"id":"LIpAqCvMYTpX"}},{"cell_type":"markdown","source":"**Dataset:** [SemEval 2017 Task](https://competitions.codalab.org/competitions/16380)\n\n**Task:** Classify text data into 4 different emotions using word embedding and other deep information retrieval approaches.\n\n![image.png](https://raw.githubusercontent.com/keziatamus/DM2022-Lab2-Master/08755efc671824064d7a9347edb8c418550c3e83//pics/pic0.png)","metadata":{"id":"n2paPeNbYTpX"}},{"cell_type":"markdown","source":"---\n# 1. Data Preparation","metadata":{"id":"op_X7pR-YTpX"}},{"cell_type":"markdown","source":"Before beggining the lab, please make sure to download the [Google News Dataset](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit) and place it in a folder named \"GoogleNews\" in the same directory as this file.","metadata":{"id":"ID-8I1ELYTpX"}},{"cell_type":"markdown","source":"## 1.1 Load data\n\nWe start by loading the csv files into a single pandas dataframe for training and one for testing.","metadata":{"id":"pgoEbZzSYTpX"}},{"cell_type":"code","source":"import pandas as pd\n\n### training data\nanger_train = pd.read_csv(\"../input/lab2-dataset/data/semeval/train/anger-ratings-0to1.train.txt\",\n                         sep=\"\\t\", header=None,names=[\"id\", \"text\", \"emotion\", \"intensity\"])\nsadness_train = pd.read_csv(\"../input/lab2-dataset/data/semeval/train/sadness-ratings-0to1.train.txt\",\n                         sep=\"\\t\", header=None, names=[\"id\", \"text\", \"emotion\", \"intensity\"])\nfear_train = pd.read_csv(\"../input/lab2-dataset/data/semeval/train/fear-ratings-0to1.train.txt\",\n                         sep=\"\\t\", header=None, names=[\"id\", \"text\", \"emotion\", \"intensity\"])\njoy_train = pd.read_csv(\"../input/lab2-dataset/data/semeval/train/joy-ratings-0to1.train.txt\",\n                         sep=\"\\t\", header=None, names=[\"id\", \"text\", \"emotion\", \"intensity\"])","metadata":{"id":"anfjcPSSYTpX","execution":{"iopub.status.busy":"2022-11-20T17:20:18.205967Z","iopub.execute_input":"2022-11-20T17:20:18.206351Z","iopub.status.idle":"2022-11-20T17:20:18.232768Z","shell.execute_reply.started":"2022-11-20T17:20:18.206320Z","shell.execute_reply":"2022-11-20T17:20:18.231787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# combine 4 sub-dataset\ntrain_df = pd.concat([anger_train, fear_train, joy_train, sadness_train], ignore_index=True)","metadata":{"id":"yVc2T5MIYTpX","execution":{"iopub.status.busy":"2022-11-20T17:20:18.234471Z","iopub.execute_input":"2022-11-20T17:20:18.234779Z","iopub.status.idle":"2022-11-20T17:20:18.243074Z","shell.execute_reply.started":"2022-11-20T17:20:18.234751Z","shell.execute_reply":"2022-11-20T17:20:18.242262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### testing data\nanger_test = pd.read_csv(\"../input/lab2-dataset/data/semeval/dev/anger-ratings-0to1.dev.gold.txt\",\n                         sep=\"\\t\", header=None,names=[\"id\", \"text\", \"emotion\", \"intensity\"])\nsadness_test = pd.read_csv(\"../input/lab2-dataset/data/semeval/dev/sadness-ratings-0to1.dev.gold.txt\",\n                         sep=\"\\t\", header=None, names=[\"id\", \"text\", \"emotion\", \"intensity\"])\nfear_test = pd.read_csv(\"../input/lab2-dataset/data/semeval/dev/fear-ratings-0to1.dev.gold.txt\",\n                         sep=\"\\t\", header=None, names=[\"id\", \"text\", \"emotion\", \"intensity\"])\njoy_test = pd.read_csv(\"../input/lab2-dataset/data/semeval/dev/joy-ratings-0to1.dev.gold.txt\",\n                         sep=\"\\t\", header=None, names=[\"id\", \"text\", \"emotion\", \"intensity\"])\n\n# combine 4 sub-dataset\ntest_df = pd.concat([anger_test, fear_test, joy_test, sadness_test], ignore_index=True)\ntrain_df.head()","metadata":{"id":"Kw8bGMv7YTpX","outputId":"9f6f7052-302e-4794-ef69-b84450b61b36","execution":{"iopub.status.busy":"2022-11-20T17:20:18.244481Z","iopub.execute_input":"2022-11-20T17:20:18.245118Z","iopub.status.idle":"2022-11-20T17:20:18.313163Z","shell.execute_reply.started":"2022-11-20T17:20:18.245087Z","shell.execute_reply":"2022-11-20T17:20:18.312408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# shuffle dataset\ntrain_df = train_df.sample(frac=1)\ntest_df = test_df.sample(frac=1)","metadata":{"id":"HBHwcL8sYTpX","execution":{"iopub.status.busy":"2022-11-20T17:20:18.314387Z","iopub.execute_input":"2022-11-20T17:20:18.314858Z","iopub.status.idle":"2022-11-20T17:20:18.323162Z","shell.execute_reply.started":"2022-11-20T17:20:18.314829Z","shell.execute_reply":"2022-11-20T17:20:18.322278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Shape of Training df: \", train_df.shape)\nprint(\"Shape of Testing df: \", test_df.shape)","metadata":{"id":"9w_cDUwCYTpX","outputId":"3582ac44-1f5f-4cb2-b833-d477f152461a","scrolled":true,"execution":{"iopub.status.busy":"2022-11-20T17:20:18.325411Z","iopub.execute_input":"2022-11-20T17:20:18.325945Z","iopub.status.idle":"2022-11-20T17:20:18.331730Z","shell.execute_reply.started":"2022-11-20T17:20:18.325914Z","shell.execute_reply":"2022-11-20T17:20:18.330599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n### ** >>> Exercise 1 (Take home): **  \nPlot word frequency for Top 30 words in both train and test dataset. (Hint: refer to DM lab 1)\n","metadata":{"id":"escCgU1zYTpX"}},{"cell_type":"code","source":"# Answer here\n\n#Training set\n\nfrom nltk.tokenize import word_tokenize\ntrain_words = train_df['text'].apply(lambda x: word_tokenize(x)).to_list()\n\ntrain_words_flat = []\nfor sublist in train_words:\n    for item in sublist:\n        train_words_flat.append(item)\n        \nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\nctr_train_30 = Counter(train_words_flat).most_common(30)\nplt.bar(*zip(*ctr_train_30))\nplt.xticks(rotation = 90)\nplt.show\n","metadata":{"id":"HoXjet3pYTpo","execution":{"iopub.status.busy":"2022-11-20T17:20:18.332942Z","iopub.execute_input":"2022-11-20T17:20:18.333325Z","iopub.status.idle":"2022-11-20T17:20:20.748450Z","shell.execute_reply.started":"2022-11-20T17:20:18.333280Z","shell.execute_reply":"2022-11-20T17:20:20.747323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Testing set\n\ntest_words = test_df['text'].apply(lambda x: word_tokenize(x)).to_list()\n\ntest_words_flat = []\nfor sublist in test_words:\n    for item in sublist:\n        test_words_flat.append(item)\n        \nctr_test_30 = Counter(test_words_flat).most_common(30)\nplt.bar(*zip(*ctr_test_30))\nplt.xticks(rotation = 90)\nplt.show","metadata":{"execution":{"iopub.status.busy":"2022-11-20T17:20:20.749788Z","iopub.execute_input":"2022-11-20T17:20:20.750105Z","iopub.status.idle":"2022-11-20T17:20:21.198654Z","shell.execute_reply.started":"2022-11-20T17:20:20.750076Z","shell.execute_reply":"2022-11-20T17:20:21.197543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n### 1.2 Save data","metadata":{"id":"_hr8aKhlYTpo"}},{"cell_type":"markdown","source":"We will save our data in Pickle format. The pickle module implements binary protocols for serializing and de-serializing a Python object structure.   \n  \nSome advantages for using pickle structure:  \n* Because it stores the attribute type, it's more convenient for cross-platform use.  \n* When your data is huge, it could use less space to store also consume less loading time.   ","metadata":{"id":"Zm6GF2VvYTpo"}},{"cell_type":"code","source":"## save to pickle file\ntrain_df.to_pickle(\"train_df.pkl\") \ntest_df.to_pickle(\"test_df.pkl\")","metadata":{"id":"dZzepBdpYTpo","execution":{"iopub.status.busy":"2022-11-20T17:20:21.199927Z","iopub.execute_input":"2022-11-20T17:20:21.200296Z","iopub.status.idle":"2022-11-20T17:20:21.208638Z","shell.execute_reply.started":"2022-11-20T17:20:21.200264Z","shell.execute_reply":"2022-11-20T17:20:21.207524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\n## load a pickle file\ntrain_df = pd.read_pickle(\"train_df.pkl\")\ntest_df = pd.read_pickle(\"test_df.pkl\")","metadata":{"id":"H5uO-kOUYTpo","execution":{"iopub.status.busy":"2022-11-20T17:20:21.211467Z","iopub.execute_input":"2022-11-20T17:20:21.211773Z","iopub.status.idle":"2022-11-20T17:20:21.221595Z","shell.execute_reply.started":"2022-11-20T17:20:21.211746Z","shell.execute_reply":"2022-11-20T17:20:21.220639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For more information: https://reurl.cc/0Dzqx","metadata":{"id":"_sLDcQzeYTpo"}},{"cell_type":"markdown","source":"---\n### 1.3 Exploratory data analysis (EDA)\n\nAgain, before getting our hands dirty, we need to explore a little bit and understand the data we're dealing with.","metadata":{"id":"dKHpxTzLYTpo"}},{"cell_type":"code","source":"#group to find distribution\ntrain_df.groupby(['emotion']).count()['text']","metadata":{"id":"mLnEEliCYTpo","outputId":"a253199b-cf8f-4cdc-b677-732be78993a3","execution":{"iopub.status.busy":"2022-11-20T17:20:21.222895Z","iopub.execute_input":"2022-11-20T17:20:21.223870Z","iopub.status.idle":"2022-11-20T17:20:21.244544Z","shell.execute_reply.started":"2022-11-20T17:20:21.223826Z","shell.execute_reply":"2022-11-20T17:20:21.243329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%matplotlib inline\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# the histogram of the data\nlabels = train_df['emotion'].unique()\npost_total = len(train_df)\ndf1 = train_df.groupby(['emotion']).count()['text']\ndf1 = df1.apply(lambda x: round(x*100/post_total,3))\n\n#plot\nfig, ax = plt.subplots(figsize=(5,3))\nplt.bar(df1.index,df1.values)\n\n#arrange\nplt.ylabel('% of instances')\nplt.xlabel('Emotion')\nplt.title('Emotion distribution')\nplt.grid(True)\nplt.show()","metadata":{"id":"pcVOe8nYYTpo","outputId":"52c413c9-5a8f-450b-d99a-e541a76e95c7","execution":{"iopub.status.busy":"2022-11-20T17:20:21.245989Z","iopub.execute_input":"2022-11-20T17:20:21.246319Z","iopub.status.idle":"2022-11-20T17:20:21.445916Z","shell.execute_reply.started":"2022-11-20T17:20:21.246291Z","shell.execute_reply":"2022-11-20T17:20:21.444873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{"id":"B_jGcireYTpo"}},{"cell_type":"markdown","source":"# 2. Feature engineering\n### Using Bag of Words\nUsing scikit-learn ```CountVectorizer``` perform word frequency and use these as features to train a model.  \nhttp://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html","metadata":{"id":"hgHvhTJuYTpo"}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer","metadata":{"id":"rbl89LPUYTpo","execution":{"iopub.status.busy":"2022-11-20T17:20:21.447190Z","iopub.execute_input":"2022-11-20T17:20:21.447520Z","iopub.status.idle":"2022-11-20T17:20:21.452746Z","shell.execute_reply.started":"2022-11-20T17:20:21.447490Z","shell.execute_reply":"2022-11-20T17:20:21.451362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# build analyzers (bag-of-words)\nBOW_vectorizer = CountVectorizer() ","metadata":{"id":"Bo8_GP6qYTpo","execution":{"iopub.status.busy":"2022-11-20T17:20:21.454190Z","iopub.execute_input":"2022-11-20T17:20:21.454522Z","iopub.status.idle":"2022-11-20T17:20:21.464092Z","shell.execute_reply.started":"2022-11-20T17:20:21.454493Z","shell.execute_reply":"2022-11-20T17:20:21.462945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 1. Learn a vocabulary dictionary of all tokens in the raw documents.\nBOW_vectorizer.fit(train_df['text'])\n\n# 2. Transform documents to document-term matrix.\ntrain_data_BOW_features = BOW_vectorizer.transform(train_df['text'])\ntest_data_BOW_features = BOW_vectorizer.transform(test_df['text'])","metadata":{"id":"Bz_m0xn7YTpo","execution":{"iopub.status.busy":"2022-11-20T17:20:21.465596Z","iopub.execute_input":"2022-11-20T17:20:21.465920Z","iopub.status.idle":"2022-11-20T17:20:21.623075Z","shell.execute_reply.started":"2022-11-20T17:20:21.465892Z","shell.execute_reply":"2022-11-20T17:20:21.621975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check the result\ntrain_data_BOW_features","metadata":{"id":"5cpCUVN8YTpo","outputId":"c68d1f47-143e-4e56-c9bd-049c9c204e11","execution":{"iopub.status.busy":"2022-11-20T17:20:21.624688Z","iopub.execute_input":"2022-11-20T17:20:21.624995Z","iopub.status.idle":"2022-11-20T17:20:21.631663Z","shell.execute_reply.started":"2022-11-20T17:20:21.624968Z","shell.execute_reply":"2022-11-20T17:20:21.630503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"type(train_data_BOW_features)","metadata":{"id":"irGLsag-YTpo","outputId":"27e90366-d681-43ef-ab5f-7c3360b1a671","execution":{"iopub.status.busy":"2022-11-20T17:20:21.633196Z","iopub.execute_input":"2022-11-20T17:20:21.633642Z","iopub.status.idle":"2022-11-20T17:20:21.645271Z","shell.execute_reply.started":"2022-11-20T17:20:21.633601Z","shell.execute_reply":"2022-11-20T17:20:21.644283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# add .toarray() to show\ntrain_data_BOW_features.toarray()","metadata":{"id":"WqbR8KWNYTpo","outputId":"50438b2b-731e-4031-8dfe-19d6c7831545","scrolled":true,"execution":{"iopub.status.busy":"2022-11-20T17:20:21.646358Z","iopub.execute_input":"2022-11-20T17:20:21.646658Z","iopub.status.idle":"2022-11-20T17:20:21.759600Z","shell.execute_reply.started":"2022-11-20T17:20:21.646629Z","shell.execute_reply":"2022-11-20T17:20:21.758451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check the dimension\ntrain_data_BOW_features.shape","metadata":{"id":"mL0xkGyGYTpo","outputId":"c6245d02-a095-44d9-b00f-bb62936bee4a","execution":{"iopub.status.busy":"2022-11-20T17:20:21.760734Z","iopub.execute_input":"2022-11-20T17:20:21.761020Z","iopub.status.idle":"2022-11-20T17:20:21.766586Z","shell.execute_reply.started":"2022-11-20T17:20:21.760994Z","shell.execute_reply":"2022-11-20T17:20:21.765482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# observe some feature names\nfeature_names = BOW_vectorizer.get_feature_names_out()\nfeature_names[100:110]","metadata":{"id":"iyRaxyBZYTpo","outputId":"f1df1307-12a0-49b7-afcb-7ad840f886ca","execution":{"iopub.status.busy":"2022-11-20T17:20:21.771616Z","iopub.execute_input":"2022-11-20T17:20:21.771984Z","iopub.status.idle":"2022-11-20T17:20:21.786941Z","shell.execute_reply.started":"2022-11-20T17:20:21.771953Z","shell.execute_reply":"2022-11-20T17:20:21.786188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" ","metadata":{"id":"zm00p_sxYTpo"}},{"cell_type":"markdown","source":"The embedding is done. We can technically feed this into our model. However, depending on the embedding technique you use and your model, your accuracy might not be as high, because:\n\n* curse of dimensionality  (we have 10,115 dimension now)\n* some important features are ignored (for example, some models using emoticons yeld better performance than counterparts)","metadata":{"id":"roSfgQKaYTpo"}},{"cell_type":"code","source":"\"ðŸ˜‚\" in feature_names","metadata":{"id":"kx4YPbrdYTpo","outputId":"3d8a8808-fb86-4fe6-93b5-93ba0124f845","execution":{"iopub.status.busy":"2022-11-20T17:20:21.788070Z","iopub.execute_input":"2022-11-20T17:20:21.788544Z","iopub.status.idle":"2022-11-20T17:20:21.799067Z","shell.execute_reply.started":"2022-11-20T17:20:21.788513Z","shell.execute_reply":"2022-11-20T17:20:21.798234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's try using another tokenizer below.","metadata":{"id":"0MFzyA95YTpo"}},{"cell_type":"code","source":"import nltk\n\n# build analyzers (bag-of-words)\nBOW_500 = CountVectorizer(max_features=500, tokenizer=nltk.word_tokenize) \n\n# apply analyzer to training data\nBOW_500.fit(train_df['text'])\n\ntrain_data_BOW_features_500 = BOW_500.transform(train_df['text'])\n\n## check dimension\ntrain_data_BOW_features_500.shape","metadata":{"id":"SttodxACYTpo","outputId":"11fe3750-22b7-45bf-c15f-586d74c35c13","execution":{"iopub.status.busy":"2022-11-20T17:20:21.800344Z","iopub.execute_input":"2022-11-20T17:20:21.801363Z","iopub.status.idle":"2022-11-20T17:20:23.633330Z","shell.execute_reply.started":"2022-11-20T17:20:21.801327Z","shell.execute_reply":"2022-11-20T17:20:23.632167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data_BOW_features_500.toarray()","metadata":{"id":"sPi42W0pYTpo","outputId":"54682edb-fa13-4292-fa48-ae81cd08b14f","execution":{"iopub.status.busy":"2022-11-20T17:20:23.634864Z","iopub.execute_input":"2022-11-20T17:20:23.635349Z","iopub.status.idle":"2022-11-20T17:20:23.655045Z","shell.execute_reply.started":"2022-11-20T17:20:23.635306Z","shell.execute_reply":"2022-11-20T17:20:23.653829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# observe some feature names\nfeature_names_500 = BOW_500.get_feature_names_out()\nfeature_names_500[100:110]","metadata":{"id":"JCgAnTOfYTpo","outputId":"bba03b17-8ca5-4942-a78f-df389c70cd1f","execution":{"iopub.status.busy":"2022-11-20T17:20:23.656330Z","iopub.execute_input":"2022-11-20T17:20:23.656658Z","iopub.status.idle":"2022-11-20T17:20:23.664537Z","shell.execute_reply.started":"2022-11-20T17:20:23.656628Z","shell.execute_reply":"2022-11-20T17:20:23.663397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"ðŸ˜‚\" in feature_names_500","metadata":{"id":"ubgedNi4YTpo","outputId":"d4c8c862-7507-497c-fc76-a2730996bb40","execution":{"iopub.status.busy":"2022-11-20T17:20:23.666244Z","iopub.execute_input":"2022-11-20T17:20:23.666892Z","iopub.status.idle":"2022-11-20T17:20:23.676286Z","shell.execute_reply.started":"2022-11-20T17:20:23.666849Z","shell.execute_reply":"2022-11-20T17:20:23.675377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n### ** >>> Exercise 2 (Take home): **  \nGenerate an embedding using the TF-IDF vectorizer instead of th BOW one with 1000 features and show the feature names for features [100:110].","metadata":{"id":"fj6TV4ngYTpo"}},{"cell_type":"code","source":"# Answer here\nfrom sklearn.feature_extraction.text import TfidfVectorizer","metadata":{"id":"BOjVbgmxYTpo","execution":{"iopub.status.busy":"2022-11-20T17:20:23.678061Z","iopub.execute_input":"2022-11-20T17:20:23.678412Z","iopub.status.idle":"2022-11-20T17:20:23.686289Z","shell.execute_reply.started":"2022-11-20T17:20:23.678381Z","shell.execute_reply":"2022-11-20T17:20:23.685193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TFIDF_Vectorizer = TfidfVectorizer(max_features=1000,tokenizer=nltk.word_tokenize)","metadata":{"execution":{"iopub.status.busy":"2022-11-20T17:20:23.687680Z","iopub.execute_input":"2022-11-20T17:20:23.688238Z","iopub.status.idle":"2022-11-20T17:20:23.701219Z","shell.execute_reply.started":"2022-11-20T17:20:23.688192Z","shell.execute_reply":"2022-11-20T17:20:23.699278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 1. Learn a vocabulary dictionary of all tokens in the raw documents.\nTFIDF_Vectorizer.fit(train_df['text'])\n\n# 2. Transform documents to document-term matrix.\ntrain_data_TFIDF_features = TFIDF_Vectorizer.transform(train_df['text']).toarray()\ntest_data_TFIDF_features = TFIDF_Vectorizer.transform(test_df['text']).toarray()","metadata":{"execution":{"iopub.status.busy":"2022-11-20T17:20:23.702581Z","iopub.execute_input":"2022-11-20T17:20:23.703512Z","iopub.status.idle":"2022-11-20T17:20:25.620487Z","shell.execute_reply.started":"2022-11-20T17:20:23.703477Z","shell.execute_reply":"2022-11-20T17:20:25.619230Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_names_TFIDF = TFIDF_Vectorizer.get_feature_names_out()\n\n#show the feature names for features [100:110]\nfeature_names_TFIDF[100:110]","metadata":{"execution":{"iopub.status.busy":"2022-11-20T17:20:25.623093Z","iopub.execute_input":"2022-11-20T17:20:25.623493Z","iopub.status.idle":"2022-11-20T17:20:25.632319Z","shell.execute_reply.started":"2022-11-20T17:20:25.623459Z","shell.execute_reply":"2022-11-20T17:20:25.631068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n# 3. Model\n### 3.1 Decision Trees\nUsing scikit-learn ```DecisionTreeClassifier``` performs word frequency and uses these as features to train a model.  \nhttp://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier","metadata":{"id":"e0BvbNAVYTpo"}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\n\n# for a classificaiton problem, you need to provide both training & testing data\nX_train = BOW_500.transform(train_df['text'])\ny_train = train_df['emotion']\n\nX_test = BOW_500.transform(test_df['text'])\ny_test = test_df['emotion']\n\n## take a look at data dimension is a good habbit  :)\nprint('X_train.shape: ', X_train.shape)\nprint('y_train.shape: ', y_train.shape)\nprint('X_test.shape: ', X_test.shape)\nprint('y_test.shape: ', y_test.shape)","metadata":{"id":"SD0rMWKgYTpo","outputId":"36873985-dd8e-40ac-8ff8-f297a21f69db","execution":{"iopub.status.busy":"2022-11-20T17:20:25.634405Z","iopub.execute_input":"2022-11-20T17:20:25.635393Z","iopub.status.idle":"2022-11-20T17:20:26.684248Z","shell.execute_reply.started":"2022-11-20T17:20:25.635347Z","shell.execute_reply":"2022-11-20T17:20:26.683004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## build DecisionTree model\nDT_model = DecisionTreeClassifier(random_state=0)\n\n## training!\nDT_model = DT_model.fit(X_train, y_train)\n\n## predict!\ny_train_pred = DT_model.predict(X_train)\ny_test_pred = DT_model.predict(X_test)\n\n## so we get the pred result\ny_test_pred[:10]","metadata":{"id":"iDuvLf7TYTpo","outputId":"892ea486-fe16-49fc-a57a-b507924d9549","execution":{"iopub.status.busy":"2022-11-20T17:20:26.685777Z","iopub.execute_input":"2022-11-20T17:20:26.687132Z","iopub.status.idle":"2022-11-20T17:20:26.914629Z","shell.execute_reply.started":"2022-11-20T17:20:26.687078Z","shell.execute_reply":"2022-11-20T17:20:26.913269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" ","metadata":{"id":"zqZHlDjxYTpo"}},{"cell_type":"markdown","source":"---\n# 4. Results Evaluation","metadata":{"id":"iBNmBT50YTpo"}},{"cell_type":"markdown","source":"Now we will check the results of our model's performance","metadata":{"id":"-gQU_PbhYTpo"}},{"cell_type":"code","source":"## accuracy\nfrom sklearn.metrics import accuracy_score\n\nacc_train = accuracy_score(y_true=y_train, y_pred=y_train_pred)\nacc_test = accuracy_score(y_true=y_test, y_pred=y_test_pred)\n\nprint('training accuracy: {}'.format(round(acc_train, 2)))\nprint('testing accuracy: {}'.format(round(acc_test, 2)))\n","metadata":{"id":"A9yx3tv-YTpo","outputId":"0b1e4f04-d7a5-4e0d-8351-66c2b802cc42","execution":{"iopub.status.busy":"2022-11-20T17:20:26.916254Z","iopub.execute_input":"2022-11-20T17:20:26.916726Z","iopub.status.idle":"2022-11-20T17:20:26.928836Z","shell.execute_reply.started":"2022-11-20T17:20:26.916682Z","shell.execute_reply":"2022-11-20T17:20:26.927691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## precision, recall, f1-score,\nfrom sklearn.metrics import classification_report\n\nprint(classification_report(y_true=y_test, y_pred=y_test_pred))","metadata":{"id":"-wkOqjqiYTpo","outputId":"f0538316-514a-4894-fb0f-ccc73204f598","execution":{"iopub.status.busy":"2022-11-20T17:20:26.930270Z","iopub.execute_input":"2022-11-20T17:20:26.930676Z","iopub.status.idle":"2022-11-20T17:20:26.952244Z","shell.execute_reply.started":"2022-11-20T17:20:26.930633Z","shell.execute_reply":"2022-11-20T17:20:26.951003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## check by confusion matrix\nfrom sklearn.metrics import confusion_matrix\n\ncm = confusion_matrix(y_true=y_test, y_pred=y_test_pred) \nprint(cm)","metadata":{"id":"N6mhrmKHYTpo","outputId":"30efb505-d5e0-41f6-e72a-05302135a4db","execution":{"iopub.status.busy":"2022-11-20T17:20:26.953911Z","iopub.execute_input":"2022-11-20T17:20:26.954410Z","iopub.status.idle":"2022-11-20T17:20:26.963681Z","shell.execute_reply.started":"2022-11-20T17:20:26.954366Z","shell.execute_reply":"2022-11-20T17:20:26.962482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Funciton for visualizing confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport itertools\n\ndef plot_confusion_matrix(cm, classes, title='Confusion matrix',\n                          cmap=sns.cubehelix_palette(as_cmap=True)):\n    \"\"\"\n    This function is modified from: \n    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n    \"\"\"\n    classes.sort()\n    tick_marks = np.arange(len(classes))    \n    \n    fig, ax = plt.subplots(figsize=(5,5))\n    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n    ax.figure.colorbar(im, ax=ax)\n    ax.set(xticks=np.arange(cm.shape[1]),\n           yticks=np.arange(cm.shape[0]),\n           xticklabels = classes,\n           yticklabels = classes,\n           title = title,\n           xlabel = 'True label',\n           ylabel = 'Predicted label')\n\n    fmt = 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n    ylim_top = len(classes) - 0.5\n    plt.ylim([ylim_top, -.5])\n    plt.tight_layout()\n    plt.show()","metadata":{"id":"-TcX8NA5YTpo","execution":{"iopub.status.busy":"2022-11-20T17:20:26.965126Z","iopub.execute_input":"2022-11-20T17:20:26.965462Z","iopub.status.idle":"2022-11-20T17:20:27.144040Z","shell.execute_reply.started":"2022-11-20T17:20:26.965434Z","shell.execute_reply":"2022-11-20T17:20:27.143116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot your confusion matrix\nmy_tags = ['anger', 'fear', 'joy', 'sadness']\nplot_confusion_matrix(cm, classes=my_tags, title='Confusion matrix')","metadata":{"id":"1nBVOUpDYTpo","outputId":"7f99188c-1f03-462e-971e-0c8238f4f0d7","execution":{"iopub.status.busy":"2022-11-20T17:20:27.145124Z","iopub.execute_input":"2022-11-20T17:20:27.145430Z","iopub.status.idle":"2022-11-20T17:20:27.461165Z","shell.execute_reply.started":"2022-11-20T17:20:27.145402Z","shell.execute_reply":"2022-11-20T17:20:27.460264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n### ** >>> Exercise 3 (Take home): **  \nCan you interpret the results above? What do they mean?","metadata":{"id":"l743vmwgYTpo"}},{"cell_type":"markdown","source":"# Answer here\nFrom the matrix above, since the data sample number is not the same, we should interpret it from the classification report instead. Looking at the precision, it can be seen that the \"joy\" sentiment of the text is the most correctly predicted sentiment, while the \"anger\" sentiment is the least correctly predicted sentiment. This implies that using the decision tree method, \"joy\" is the most accurate to predict based on the words in the text using the algorithm, while \"anger\" is the least.\n\nAlso, it also seems like the algorithm confused between \"anger\" and \"fear\" the most out of all the combinations of possible sentiments.","metadata":{"id":"8pYICOxsYTpo","execution":{"iopub.status.busy":"2022-11-20T04:23:45.985587Z","iopub.execute_input":"2022-11-20T04:23:45.985950Z","iopub.status.idle":"2022-11-20T04:23:45.990944Z","shell.execute_reply.started":"2022-11-20T04:23:45.985916Z","shell.execute_reply":"2022-11-20T04:23:45.989779Z"}}},{"cell_type":"markdown","source":"---\n### ** >>> Exercise 4 (Take home): **  \nBuild a model using a ```Naive Bayes``` model and train it. What are the testing results? \n\n*Reference*: https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html","metadata":{"id":"GaHpgl87YTpo"}},{"cell_type":"code","source":"# Answer here\nfrom sklearn.naive_bayes import MultinomialNB\n\n## build NB model\nNB_model = MultinomialNB()\n\n## training!\nNB_model = NB_model.fit(X_train, y_train)\n\n## predict!\ny_train_pred_NB = NB_model.predict(X_train)\ny_test_pred_NB = NB_model.predict(X_test)","metadata":{"id":"ZPvaHzpXYTpo","execution":{"iopub.status.busy":"2022-11-20T17:20:27.462812Z","iopub.execute_input":"2022-11-20T17:20:27.463584Z","iopub.status.idle":"2022-11-20T17:20:27.484244Z","shell.execute_reply.started":"2022-11-20T17:20:27.463538Z","shell.execute_reply":"2022-11-20T17:20:27.483047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"acc_train_NB = accuracy_score(y_true=y_train, y_pred=y_train_pred_NB)\nacc_test_NB = accuracy_score(y_true=y_test, y_pred=y_test_pred_NB)\n\nprint('training accuracy: {}'.format(round(acc_train_NB, 2)))\nprint('testing accuracy: {}'.format(round(acc_test_NB, 2)))","metadata":{"execution":{"iopub.status.busy":"2022-11-20T17:20:27.485791Z","iopub.execute_input":"2022-11-20T17:20:27.486909Z","iopub.status.idle":"2022-11-20T17:20:27.497164Z","shell.execute_reply.started":"2022-11-20T17:20:27.486871Z","shell.execute_reply":"2022-11-20T17:20:27.496007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_true=y_test, y_pred=y_test_pred_NB))","metadata":{"execution":{"iopub.status.busy":"2022-11-20T17:20:27.498512Z","iopub.execute_input":"2022-11-20T17:20:27.498841Z","iopub.status.idle":"2022-11-20T17:20:27.513196Z","shell.execute_reply.started":"2022-11-20T17:20:27.498811Z","shell.execute_reply":"2022-11-20T17:20:27.512435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cm_NB = confusion_matrix(y_true=y_test, y_pred=y_test_pred_NB) \nprint(cm_NB)","metadata":{"execution":{"iopub.status.busy":"2022-11-20T17:20:27.514392Z","iopub.execute_input":"2022-11-20T17:20:27.514863Z","iopub.status.idle":"2022-11-20T17:20:27.521453Z","shell.execute_reply.started":"2022-11-20T17:20:27.514832Z","shell.execute_reply":"2022-11-20T17:20:27.520441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_confusion_matrix(cm_NB, classes=my_tags, title='Confusion matrix')","metadata":{"execution":{"iopub.status.busy":"2022-11-20T17:20:27.522773Z","iopub.execute_input":"2022-11-20T17:20:27.523131Z","iopub.status.idle":"2022-11-20T17:20:27.983381Z","shell.execute_reply.started":"2022-11-20T17:20:27.523088Z","shell.execute_reply":"2022-11-20T17:20:27.982258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n### ** >>> Exercise 5 (Take home): **  \n\nHow do the results from the Naive Bayes model and the Decision Tree model compare? How do you interpret these differences? Use the theoretical background covered in class to try and explain these differences.","metadata":{"id":"Xv2DqWQSYTpo"}},{"cell_type":"markdown","source":"# Answer here\nFrom the accuracy values (specifically the testing value), it can be seen that the Naive Bayes classifier has a better accuracy than the Decision Tree. The Naive Bayes classifier also generally has better average precision and recall values, as well as a better overall F1-Score across the 4 emotions.\n\nFrom the confusion matrix, more texts are correctly predicted in the NB classifier. The most notable change is the less number of confused predictions between the anger and fear emotions.","metadata":{"id":"ALN_jHdlYTpo","execution":{"iopub.status.busy":"2022-11-20T04:27:28.344893Z","iopub.execute_input":"2022-11-20T04:27:28.345270Z","iopub.status.idle":"2022-11-20T04:27:28.349917Z","shell.execute_reply.started":"2022-11-20T04:27:28.345235Z","shell.execute_reply":"2022-11-20T04:27:28.348697Z"}}},{"cell_type":"markdown","source":"---","metadata":{"id":"ehlJ60lhYTpo"}},{"cell_type":"markdown","source":"## 5. Other things you can try","metadata":{"id":"79F_DaW-YTpo"}},{"cell_type":"markdown","source":"Thus, there are several things you can try that will affect your results. In order to yield better results, you can experiment by: \n- Trying different features (Feature engineering), such as: Word2Vec,PCA,LDA,FastText, Clustering\n- Trying different models\n- Analyzing your results and interpret them to improve your feature engineering/model building process\n- Iterate through the steps above until finding a satisfying result\nRemember that you should also consider the task at hand and the model you'll feed the data to. ","metadata":{"id":"_oeqpRu6YTpo"}},{"cell_type":"markdown","source":"---\n# 6. Deep Learning\n\nWe use [Keras](https://keras.io/) to be our deep learning framwork, and follow the [Model (functional API)](https://keras.io/models/model/) to build a Deep Neural Network (DNN) model. Keras runs with Tensorflow in the backend. It's a nice abstraction to start working with NN models. \n\nBecause Deep Learning is a 1-semester course, we can't talk about each detail about it in the lab session. Here, we only provide a simple template about how to build & run a DL model successfully. You can follow this template to design your model.\n\nWe will begin by building a fully connected network, which looks like this:","metadata":{"id":"fiGUSmPLYTpo"}},{"cell_type":"markdown","source":"![image.png](https://raw.githubusercontent.com/keziatamus/DM2022-Lab2-Master/08755efc671824064d7a9347edb8c418550c3e83//pics/pic1.png)","metadata":{"id":"1nB0BTq2YTpo"}},{"cell_type":"markdown","source":"### 6.1 Prepare data (X, y)","metadata":{"id":"-EtVRGhNYTpo"}},{"cell_type":"code","source":"import keras\n\n# standardize name (X, y) \nX_train = BOW_500.transform(train_df['text'])\ny_train = train_df['emotion']\n\nX_test = BOW_500.transform(test_df['text'])\ny_test = test_df['emotion']\n\n## check dimension is a good habbit \nprint('X_train.shape: ', X_train.shape)\nprint('y_train.shape: ', y_train.shape)\nprint('X_test.shape: ', X_test.shape)\nprint('y_test.shape: ', y_test.shape)","metadata":{"id":"4mIdg2D6YTpo","outputId":"8f4fa078-7e07-4b76-e5ee-163d123abde3","execution":{"iopub.status.busy":"2022-11-20T17:20:27.984628Z","iopub.execute_input":"2022-11-20T17:20:27.984942Z","iopub.status.idle":"2022-11-20T17:20:34.776149Z","shell.execute_reply.started":"2022-11-20T17:20:27.984914Z","shell.execute_reply":"2022-11-20T17:20:34.774975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 6.2 Deal with categorical label (y)\n\nRather than put your label `train_df['emotion']` directly into a model, we have to process these categorical (or say nominal) label by ourselves. \n\nHere, we use the basic method [one-hot encoding](https://en.wikipedia.org/wiki/One-hot) to transform our categorical  labels to numerical ones.\n","metadata":{"id":"iBZZedZ2YTpo"}},{"cell_type":"code","source":"## deal with label (string -> one-hot)\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom keras.utils import np_utils\n\nlabel_encoder = LabelEncoder()\nlabel_encoder.fit(y_train)\nprint('check label: ', label_encoder.classes_)\nprint('\\n## Before convert')\nprint('y_train[0:4]:\\n', y_train[0:4])\nprint('\\ny_train.shape: ', y_train.shape)\nprint('y_test.shape: ', y_test.shape)\n\ndef label_encode(le, labels):\n    enc = le.transform(labels)\n    return keras.utils.np_utils.to_categorical(enc)\n\ndef label_decode(le, one_hot_label):\n    dec = np.argmax(one_hot_label, axis=1)\n    return le.inverse_transform(dec)\n\ny_train = label_encode(label_encoder, y_train)\ny_test = label_encode(label_encoder, y_test)\n\nprint('\\n\\n## After convert')\nprint('y_train[0:4]:\\n', y_train[0:4])\nprint('\\ny_train.shape: ', y_train.shape)\nprint('y_test.shape: ', y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2022-11-20T17:20:34.777639Z","iopub.execute_input":"2022-11-20T17:20:34.778344Z","iopub.status.idle":"2022-11-20T17:20:34.794442Z","shell.execute_reply.started":"2022-11-20T17:20:34.778307Z","shell.execute_reply":"2022-11-20T17:20:34.792879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 6.3 Build model","metadata":{"id":"W4bqEcMbYTpo"}},{"cell_type":"code","source":"# I/O check\ninput_shape = X_train.shape[1]\nprint('input_shape: ', input_shape)\n\noutput_shape = len(label_encoder.classes_)\nprint('output_shape: ', output_shape)","metadata":{"id":"6sA7cx-oYTpo","outputId":"9f544ca7-8ceb-45e4-a76d-bdc8ccdb329c","execution":{"iopub.status.busy":"2022-11-20T17:20:34.796218Z","iopub.execute_input":"2022-11-20T17:20:34.796667Z","iopub.status.idle":"2022-11-20T17:20:34.804687Z","shell.execute_reply.started":"2022-11-20T17:20:34.796625Z","shell.execute_reply":"2022-11-20T17:20:34.803591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"![image.png](https://raw.githubusercontent.com/keziatamus/DM2022-Lab2-Master/08755efc671824064d7a9347edb8c418550c3e83//pics/pic2.png)","metadata":{"id":"8c-uWuloYTpo"}},{"cell_type":"code","source":"from keras.models import Model\nfrom keras.layers import Input, Dense\nfrom keras.layers import ReLU, Softmax\n\n# input layer\nmodel_input = Input(shape=(input_shape, ))  # 500\nX = model_input\n\n# 1st hidden layer\nX_W1 = Dense(units=64)(X)  # 64\nH1 = ReLU()(X_W1)\n\n# 2nd hidden layer\nH1_W2 = Dense(units=64)(H1)  # 64\nH2 = ReLU()(H1_W2)\n\n# output layer\nH2_W3 = Dense(units=output_shape)(H2)  # 4\nH3 = Softmax()(H2_W3)\n\nmodel_output = H3\n\n# create model\nmodel = Model(inputs=[model_input], outputs=[model_output])\n\n# loss function & optimizer\nmodel.compile(optimizer='adam',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n\n# show model construction\nmodel.summary()","metadata":{"id":"jTeBWTvgYTpo","outputId":"3e88d40f-2179-40ea-89c6-55085cabd1c6","execution":{"iopub.status.busy":"2022-11-20T17:20:34.805908Z","iopub.execute_input":"2022-11-20T17:20:34.806589Z","iopub.status.idle":"2022-11-20T17:20:34.958803Z","shell.execute_reply.started":"2022-11-20T17:20:34.806556Z","shell.execute_reply":"2022-11-20T17:20:34.957603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 6.4 Train","metadata":{"id":"nmTSDO2pYTpo"}},{"cell_type":"code","source":"from keras.callbacks import CSVLogger\n\ncsv_logger = CSVLogger('training_log.csv')\n\n# training setting\nepochs = 25\nbatch_size = 32\n\n# training!\nhistory = model.fit(X_train, y_train, \n                    epochs=epochs, \n                    batch_size=batch_size, \n                    callbacks=[csv_logger],\n                    validation_data = (X_test, y_test))\nprint('training finish')","metadata":{"id":"Kl374LYqYTpo","outputId":"75b1f651-b779-4efd-ff14-0b4a0e95d8db","execution":{"iopub.status.busy":"2022-11-20T17:20:34.960378Z","iopub.execute_input":"2022-11-20T17:20:34.961411Z","iopub.status.idle":"2022-11-20T17:20:44.139655Z","shell.execute_reply.started":"2022-11-20T17:20:34.961365Z","shell.execute_reply":"2022-11-20T17:20:44.138613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 6.5 Predict on testing data","metadata":{"id":"ip8RYsvSYTpo"}},{"cell_type":"code","source":"## predict\npred_result = model.predict(X_test, batch_size=128)\npred_result[:5]","metadata":{"id":"xdnLuBYBYTpo","outputId":"6e7e97b7-fe5c-44f6-9baa-a71335aa7b06","execution":{"iopub.status.busy":"2022-11-20T17:20:44.141047Z","iopub.execute_input":"2022-11-20T17:20:44.141382Z","iopub.status.idle":"2022-11-20T17:20:44.287894Z","shell.execute_reply.started":"2022-11-20T17:20:44.141352Z","shell.execute_reply":"2022-11-20T17:20:44.287059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_result = label_decode(label_encoder, pred_result)\npred_result[:5]","metadata":{"id":"hSaXGEX-YTpo","outputId":"d35b5fb3-74e5-44d9-ac1d-8a11cca023e4","execution":{"iopub.status.busy":"2022-11-20T17:20:44.289187Z","iopub.execute_input":"2022-11-20T17:20:44.289720Z","iopub.status.idle":"2022-11-20T17:20:44.295560Z","shell.execute_reply.started":"2022-11-20T17:20:44.289688Z","shell.execute_reply":"2022-11-20T17:20:44.294756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n\nprint('testing accuracy: {}'.format(round(accuracy_score(label_decode(label_encoder, y_test), pred_result), 2)))","metadata":{"id":"bRRHye9KYTp5","outputId":"d4f240ef-5f59-4d90-e3a2-57b98ced3928","execution":{"iopub.status.busy":"2022-11-20T17:20:44.296885Z","iopub.execute_input":"2022-11-20T17:20:44.298012Z","iopub.status.idle":"2022-11-20T17:20:44.307823Z","shell.execute_reply.started":"2022-11-20T17:20:44.297978Z","shell.execute_reply":"2022-11-20T17:20:44.306780Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Let's take a look at the training log\ntraining_log = pd.DataFrame()\ntraining_log = pd.read_csv(\"training_log.csv\")\ntraining_log","metadata":{"id":"ks2Q0aMsYTp5","outputId":"8b802a69-6377-4f0a-c1fa-2f03e5abebd4","execution":{"iopub.status.busy":"2022-11-20T17:20:44.315503Z","iopub.execute_input":"2022-11-20T17:20:44.316336Z","iopub.status.idle":"2022-11-20T17:20:44.336956Z","shell.execute_reply.started":"2022-11-20T17:20:44.316286Z","shell.execute_reply":"2022-11-20T17:20:44.336100Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n### ** >>> Exercise 6 (Take home): **  \n\nPlot the Training and Validation Accuracy and Loss (different plots), just like the images below (Note: the pictures below are an example from a different model). How to interpret the graphs you got? How are they related to the concept of overfitting/underfitting covered in class?\n\n![image.png](https://raw.githubusercontent.com/keziatamus/DM2022-Lab2-Master/08755efc671824064d7a9347edb8c418550c3e83//pics/pic3.png)![image.png](https://raw.githubusercontent.com/keziatamus/DM2022-Lab2-Master/08755efc671824064d7a9347edb8c418550c3e83//pics/pic4.png)","metadata":{"id":"NoYqY0-tYTp5"}},{"cell_type":"code","source":"# Answer here\n\nx = training_log['epoch']\ny1 = training_log['accuracy']\ny2 = training_log['val_accuracy']\ny3 = training_log['loss']\ny4 = training_log['val_loss']\n\nplt.plot(x, y1, label = 'Train Accuracy')\nplt.plot(x, y2, label = 'Val Accuracy')\nplt.title('Training Accuracy per Epoch')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend(loc = \"upper right\")\nplt.show()","metadata":{"id":"AlhstCrlYTp5","execution":{"iopub.status.busy":"2022-11-20T17:20:44.338325Z","iopub.execute_input":"2022-11-20T17:20:44.338671Z","iopub.status.idle":"2022-11-20T17:20:44.562581Z","shell.execute_reply.started":"2022-11-20T17:20:44.338639Z","shell.execute_reply":"2022-11-20T17:20:44.561198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(x, y3, label = 'Train Loss')\nplt.plot(x, y4, label = 'Val Loss')\nplt.title('Training Loss per Epoch')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend(loc = \"upper right\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-11-20T17:20:44.564369Z","iopub.execute_input":"2022-11-20T17:20:44.564807Z","iopub.status.idle":"2022-11-20T17:20:44.791407Z","shell.execute_reply.started":"2022-11-20T17:20:44.564764Z","shell.execute_reply":"2022-11-20T17:20:44.790415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the training and validation accuracy graph,the training accuracy comes out to be nearing 1.0 while the validation accuracy is around 0.7. Since the validation accuracy is far less than the training accuracy, we can infer that the model is overfitting. This interpretation is strengthen with the training and validation loss graph. As seen, the model performs well on training data, resulting in decreasing and low training loss. However, it performs poorly on the new data in the validation set. From epoch 1~3, it can be seen the validation loss decreases, but from epoch 4 onwards, the validation loss starts to increase. This indicates that the model is overfitting and that it cannot generalize on new data. Probable causes of overfitting are the model is too complex, the size of the training dataset used is not enough, data used for training is not cleaned and contains noise in it, and that the model was trained for a long period. One of the solutions to prevent overfitting is early stopping, the act of halting training when the loss is low and stable.","metadata":{}},{"cell_type":"markdown","source":"---","metadata":{"id":"UYabzgSGYTp5"}},{"cell_type":"markdown","source":"### Note\n\nIf you don't have a GPU (level is higher than GTX 1060) or you are not good at setting lots of things about computer, we recommend you to use the [kaggle kernel](https://www.kaggle.com/kernels) to do deep learning model training. They have already installed all the librarys and provided free GPU for you to use.\n\nNote however that you will only be able to run a kernel for 6 hours. After 6 hours of inactivity, your Kaggle kernel will shut down (meaning if your model takes more than 6 hours to train, you can't train it at once).\n\n\n### More Information for your reference\n\n* Keras document: https://keras.io/\n* Keras GitHub example: https://github.com/keras-team/keras/tree/master/examples\n* CS229: Machine Learning: http://cs229.stanford.edu/syllabus.html\n* Deep Learning cheatsheet: https://stanford.edu/~shervine/teaching/cs-229/cheatsheet-deep-learning\n* If you want to try TensorFlow or PyTorch: https://pytorch.org/tutorials/\nhttps://www.tensorflow.org/tutorials/quickstart/beginner","metadata":{"id":"4e5eiVLOYTp5"}},{"cell_type":"markdown","source":"---\n# 7. Word2Vector\n\nWe will introduce how to use `gensim` to train your word2vec model and how to load a pre-trained model.\n\nhttps://radimrehurek.com/gensim/index.html","metadata":{"id":"IESBq48MYTp5"}},{"cell_type":"markdown","source":"### 7.1 Prepare training corpus","metadata":{"id":"KRSDMhQ5YTp5"}},{"cell_type":"code","source":"## check library\nimport gensim\n\n## ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# # if you want to see the training messages, you can use it\n# import logging\n# logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n\n## the input type\ntrain_df['text_tokenized'] = train_df['text'].apply(lambda x: nltk.word_tokenize(x))\ntrain_df[['id', 'text', 'text_tokenized']].head()","metadata":{"id":"6aBYrovJYTp5","outputId":"3df27e8d-0a96-40a5-8e3d-3186bb1ea624","execution":{"iopub.status.busy":"2022-11-20T17:20:44.793968Z","iopub.execute_input":"2022-11-20T17:20:44.794449Z","iopub.status.idle":"2022-11-20T17:20:45.836902Z","shell.execute_reply.started":"2022-11-20T17:20:44.794405Z","shell.execute_reply":"2022-11-20T17:20:45.835637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## create the training corpus\ntraining_corpus = train_df['text_tokenized'].values\ntraining_corpus[:3]","metadata":{"id":"okFIEcmnYTp5","outputId":"17a7ccde-3595-4837-f8d2-91698348fb22","execution":{"iopub.status.busy":"2022-11-20T17:20:45.838027Z","iopub.execute_input":"2022-11-20T17:20:45.838389Z","iopub.status.idle":"2022-11-20T17:20:45.844326Z","shell.execute_reply.started":"2022-11-20T17:20:45.838358Z","shell.execute_reply":"2022-11-20T17:20:45.843495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 7.2 Training our model\n\nYou can try to train your own model. More details: https://radimrehurek.com/gensim/models/word2vec.html","metadata":{"id":"dOgAriPRYTp5"}},{"cell_type":"code","source":"## the input type\ntrain_df['text_tokenized'] = train_df['text'].apply(lambda x: nltk.word_tokenize(x))\ntrain_df[['id', 'text', 'text_tokenized']].head()","metadata":{"id":"4kF8vvi5YTp5","outputId":"92fed35a-efbe-4af8-bf8e-03e9e6a38745","execution":{"iopub.status.busy":"2022-11-20T17:20:45.845591Z","iopub.execute_input":"2022-11-20T17:20:45.846232Z","iopub.status.idle":"2022-11-20T17:20:46.691928Z","shell.execute_reply.started":"2022-11-20T17:20:45.846183Z","shell.execute_reply":"2022-11-20T17:20:46.691044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from gensim.models import Word2Vec\n\n## setting\nvector_dim = 100\nwindow_size = 5\nmin_count = 1\ntraining_epochs = 20\n\n## model\nword2vec_model = Word2Vec(sentences=training_corpus, \n                          vector_size=vector_dim, window=window_size, \n                          min_count=min_count, epochs=training_epochs)","metadata":{"id":"72ZA54IDYTp5","scrolled":true,"execution":{"iopub.status.busy":"2022-11-20T17:20:46.693325Z","iopub.execute_input":"2022-11-20T17:20:46.693859Z","iopub.status.idle":"2022-11-20T17:20:48.479252Z","shell.execute_reply.started":"2022-11-20T17:20:46.693827Z","shell.execute_reply":"2022-11-20T17:20:48.478359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"![Imgur](https://i.imgur.com/Fca3MCs.png)","metadata":{"id":"ob0Molb3YTp5"}},{"cell_type":"markdown","source":"### 7.3 Generating word vector (embeddings)","metadata":{"id":"E0jjvjN5YTp5"}},{"cell_type":"code","source":"# get the corresponding vector of a word\nword_vec = word2vec_model.wv['happy']\nword_vec","metadata":{"id":"4ejofZfCYTp5","outputId":"b6e8683c-4368-41e4-b05b-868822b9c406","execution":{"iopub.status.busy":"2022-11-20T17:20:48.480137Z","iopub.execute_input":"2022-11-20T17:20:48.480467Z","iopub.status.idle":"2022-11-20T17:20:48.488751Z","shell.execute_reply.started":"2022-11-20T17:20:48.480437Z","shell.execute_reply":"2022-11-20T17:20:48.487692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get the most similar words\nword = 'happy'\ntopn = 10\nword2vec_model.wv.most_similar(word, topn=topn)","metadata":{"id":"9dUSkCscYTp5","outputId":"ebd32d0b-bcda-4140-805e-35dfef1a68fb","execution":{"iopub.status.busy":"2022-11-20T17:20:48.490074Z","iopub.execute_input":"2022-11-20T17:20:48.490372Z","iopub.status.idle":"2022-11-20T17:20:48.514804Z","shell.execute_reply.started":"2022-11-20T17:20:48.490344Z","shell.execute_reply":"2022-11-20T17:20:48.513080Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 7.4 Using a pre-trained w2v model\n\nInstead of training your own model ,you can use a model that has already been trained. Here, we see 2 ways of doing that:\n\n\n#### (1) Download model by yourself\n\nsource: [GoogleNews-vectors-negative300](https://code.google.com/archive/p/word2vec/)\n\nmore details: https://radimrehurek.com/gensim/models/keyedvectors.html","metadata":{"id":"zuQvZVJvYTp5"}},{"cell_type":"code","source":"from gensim.models import KeyedVectors\n\n## Note: this model is very huge, this will take some time ...\nmodel_path = \"../input/googlenews/GoogleNews-vectors-negative300.bin\"\nw2v_google_model = KeyedVectors.load_word2vec_format(model_path, binary=True)\nprint('load ok')\n\nw2v_google_model.most_similar('happy', topn=10)","metadata":{"id":"bdH9E9auYTp5","outputId":"6193f5df-93cb-4c77-96fb-2d36b5d71835","execution":{"iopub.status.busy":"2022-11-20T17:20:48.517861Z","iopub.execute_input":"2022-11-20T17:20:48.520198Z","iopub.status.idle":"2022-11-20T17:21:55.347440Z","shell.execute_reply.started":"2022-11-20T17:20:48.520135Z","shell.execute_reply":"2022-11-20T17:21:55.345961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### (2) Using gensim api\n\nOther pretrained models are available here: https://github.com/RaRe-Technologies/gensim-data","metadata":{"id":"NdQ9ul0eYTp5"}},{"cell_type":"code","source":"import gensim.downloader as api\n\n## If you see `SSL: CERTIFICATE_VERIFY_FAILED` error, use this:\nimport ssl\nimport urllib.request\nssl._create_default_https_context = ssl._create_unverified_context\n\nglove_twitter_25_model = api.load(\"glove-twitter-25\")\nprint('load ok')\n\nglove_twitter_25_model.most_similar('happy', topn=10)","metadata":{"id":"oIxHpNB6YTp5","outputId":"8dc8d6f9-80b3-4cff-810d-28f175f5b891","execution":{"iopub.status.busy":"2022-11-20T17:21:55.349723Z","iopub.execute_input":"2022-11-20T17:21:55.350640Z","iopub.status.idle":"2022-11-20T17:22:43.281448Z","shell.execute_reply.started":"2022-11-20T17:21:55.350575Z","shell.execute_reply":"2022-11-20T17:22:43.279248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 7.5 king + woman - man = ?","metadata":{"id":"GCNDNqeXYTp5"}},{"cell_type":"markdown","source":"Let's run one of the most famous examples for Word2Vec and compute the similarity between these 3 words:","metadata":{"id":"_GtCRr_7YTp5"}},{"cell_type":"code","source":"w2v_google_model.most_similar(positive=['king', 'woman'], negative=['man'])","metadata":{"id":"Zew7m_kIYTp5","outputId":"807bc2d0-c2c1-4b96-e61d-a62e6096aa1e","execution":{"iopub.status.busy":"2022-11-20T17:22:43.283947Z","iopub.execute_input":"2022-11-20T17:22:43.284785Z","iopub.status.idle":"2022-11-20T17:22:43.560337Z","shell.execute_reply.started":"2022-11-20T17:22:43.284719Z","shell.execute_reply":"2022-11-20T17:22:43.558478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n###  >>> Exercise 7 (Take home): \n\nNow, we have the word vectors, but our input data is a sequence of words (or say sentence). \nHow can we utilize these \"word\" vectors to represent the sentence data and train our model?","metadata":{}},{"cell_type":"markdown","source":"# Answer here\nWe can represent the sentence data and train our model using average of word2vec vectors. We can take the average of all the word vectors in a sentence since this average vector represents the sentence vector. Another method is to use the average of word2vec vectors with TF-IDF. We can take the word vectors, then multiplying it with their respective TF-IDF scores. The sentence vector is represented by its average. The reason why average works is because it handles variable length sentences. On the other hand, the sum(length) of sentence leaves a very strong signal and that the essence of combination of the words will be treated as noise. There is also an alternative method without using the average, which is known as doc2vec. We can train the dataset using doc2vec and then use the sentence vectors. ","metadata":{"id":"TBwRT93DYTp5","execution":{"iopub.status.busy":"2022-11-20T17:22:43.568397Z","iopub.execute_input":"2022-11-20T17:22:43.569525Z","iopub.status.idle":"2022-11-20T17:22:43.576104Z","shell.execute_reply.started":"2022-11-20T17:22:43.569454Z","shell.execute_reply":"2022-11-20T17:22:43.574449Z"}}},{"cell_type":"markdown","source":"# 8. Clustering: k-means\n\nHere we introduce how to use `sklearn` to do the basic **unsupervised learning** approach, k-means.    \n\nmore details: http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n","metadata":{"id":"LrK7O1KDYTp5"}},{"cell_type":"markdown","source":"#### Basic concept\n\n![image.png](https://camo.githubusercontent.com/630f4ff082ed061752822d94fce7b3da1f841301be6d5a3ee529d0bfe9186f87/68747470733a2f2f692e696d6775722e636f6d2f504564556635342e706e67)","metadata":{"id":"Hr8_IxwBYTp5"}},{"cell_type":"code","source":"# clustering target\ntarget_list = ['happy', 'fear', 'angry', 'car', 'teacher', 'computer']\nprint('target words: ', target_list)\n\n# convert to word vector\nX = [word2vec_model.wv[word] for word in target_list]","metadata":{"id":"6heUPVwWYTp5","outputId":"46a95152-7852-49d5-c055-f24ac1a04aa3","execution":{"iopub.status.busy":"2022-11-20T17:22:43.577475Z","iopub.execute_input":"2022-11-20T17:22:43.577827Z","iopub.status.idle":"2022-11-20T17:22:43.591223Z","shell.execute_reply.started":"2022-11-20T17:22:43.577789Z","shell.execute_reply":"2022-11-20T17:22:43.590409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.cluster import KMeans\n\n# we have to decide how many cluster (k) we want\nk = 2\n\n# k-means model\nkmeans_model = KMeans(n_clusters=k)\nkmeans_model.fit(X)\n\n# cluster result\ncluster_result = kmeans_model.labels_\n\n# show\nfor i in range(len(target_list)):\n    print('word: {} \\t cluster: {}'.format(target_list[i], cluster_result[i]))","metadata":{"id":"E9t_sJrvYTp5","outputId":"47d70112-5490-47cc-8014-1b5b7dadbb26","execution":{"iopub.status.busy":"2022-11-20T17:22:43.592742Z","iopub.execute_input":"2022-11-20T17:22:43.593328Z","iopub.status.idle":"2022-11-20T17:22:43.717125Z","shell.execute_reply.started":"2022-11-20T17:22:43.593295Z","shell.execute_reply":"2022-11-20T17:22:43.716242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"![image.png](https://raw.githubusercontent.com/keziatamus/DM2022-Lab2-Master/08755efc671824064d7a9347edb8c418550c3e83//pics/pic6.png)","metadata":{"id":"QcDTL7kRYTp5"}},{"cell_type":"code","source":"#check cluster membership\nword = 'student'\nword_vec = word2vec_model.wv[word]\nkmeans_model.predict([word_vec])","metadata":{"id":"NIMFax_uYTp5","outputId":"2f2df2f8-98d2-426b-8ced-719be6a03281","execution":{"iopub.status.busy":"2022-11-20T17:22:43.721306Z","iopub.execute_input":"2022-11-20T17:22:43.723996Z","iopub.status.idle":"2022-11-20T17:22:44.124490Z","shell.execute_reply.started":"2022-11-20T17:22:43.723951Z","shell.execute_reply":"2022-11-20T17:22:44.123038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#check cluster membership\nword = 'sad'\nword_vec = word2vec_model.wv[word]\nkmeans_model.predict([word_vec])","metadata":{"id":"vIDuLDOlYTp5","outputId":"eae37f20-1de0-498d-a327-ce60d7336fb0","execution":{"iopub.status.busy":"2022-11-20T17:22:44.126572Z","iopub.execute_input":"2022-11-20T17:22:44.126951Z","iopub.status.idle":"2022-11-20T17:22:44.153714Z","shell.execute_reply.started":"2022-11-20T17:22:44.126917Z","shell.execute_reply":"2022-11-20T17:22:44.152556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n# 9. High-dimension Visualization: t-SNE\n\nNo matter if you use the Bag-of-words, tf-idf, or word2vec, it's very hard to see the embedding result, because the dimension is larger than 3.  \n\nIn Lab 1, we already talked about PCA. We can use PCA to reduce the dimension of our data, then visualize it. However, if you dig deeper into the result, you'd find it is insufficient...\n\nOur aim will be to create a visualization similar to the one below:","metadata":{"id":"cZOEGH3GYTp5"}},{"cell_type":"markdown","source":"  ","metadata":{"id":"4FeIFzzxYTp5"}},{"cell_type":"markdown","source":"![image.png](https://raw.githubusercontent.com/keziatamus/DM2022-Lab2-Master/08755efc671824064d7a9347edb8c418550c3e83//pics/pic7.png)","metadata":{"id":"3-JR-rqyYTp5"}},{"cell_type":"markdown","source":"  ","metadata":{"id":"KCFR771SYTp5"}},{"cell_type":"markdown","source":"Here we would like to introduce another visualization method called t-SNE.  \nhttp://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html ","metadata":{"id":"tmdbJbjxYTp5"}},{"cell_type":"markdown","source":"### 9.1 Prepare visualizing target","metadata":{"id":"MU8eeDnGYTp5"}},{"cell_type":"markdown","source":"Let's repare data lists like:\n    - happpy words\n    - angry words\n    - data words\n    - mining words","metadata":{"id":"T9IHcP3VYTp5"}},{"cell_type":"code","source":"word_list = ['happy', 'angry', 'data', 'mining']\n\ntopn = 5\nhappy_words = ['happy'] + [word_ for word_, sim_ in w2v_google_model.most_similar('happy', topn=topn)]\nangry_words = ['angry'] + [word_ for word_, sim_ in w2v_google_model.most_similar('angry', topn=topn)]        \ndata_words = ['data'] + [word_ for word_, sim_ in w2v_google_model.most_similar('data', topn=topn)]        \nmining_words = ['mining'] + [word_ for word_, sim_ in w2v_google_model.most_similar('mining', topn=topn)]        \n\nprint('happy_words: ', happy_words)\nprint('angry_words: ', angry_words)\nprint('data_words: ', data_words)\nprint('mining_words: ', mining_words)\n\ntarget_words = happy_words + angry_words + data_words + mining_words\nprint('\\ntarget words: ')\nprint(target_words)\n\nprint('\\ncolor list:')\ncn = topn + 1\ncolor = ['b'] * cn + ['g'] * cn + ['r'] * cn + ['y'] * cn\nprint(color)","metadata":{"id":"L9il5L7pYTp5","outputId":"91d18f86-d137-4771-94db-ca7542d5a8d6","execution":{"iopub.status.busy":"2022-11-20T17:22:44.155955Z","iopub.execute_input":"2022-11-20T17:22:44.156439Z","iopub.status.idle":"2022-11-20T17:22:45.109945Z","shell.execute_reply.started":"2022-11-20T17:22:44.156395Z","shell.execute_reply":"2022-11-20T17:22:45.107802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 9.1 Plot using t-SNE (2-dimension)","metadata":{"id":"zKa5LRxbYTp5"}},{"cell_type":"code","source":"%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom sklearn.manifold import TSNE\n\n## w2v model\nmodel = w2v_google_model\n\n## prepare training word vectors\nsize = 200\ntarget_size = len(target_words)\nall_word = list(model.index_to_key)\nword_train = target_words + all_word[:size]\nX_train = model[word_train]\n\n## t-SNE model\ntsne = TSNE(n_components=2, metric='cosine', random_state=28)\n\n## training\nX_tsne = tsne.fit_transform(X_train)\n\n## plot the result\nplt.figure(figsize=(7.5, 7.5), dpi=115)\nplt.scatter(X_tsne[:target_size, 0], X_tsne[:target_size, 1], c=color)\nfor label, x, y in zip(target_words, X_tsne[:target_size, 0], X_tsne[:target_size, 1]):\n    plt.annotate(label, xy=(x,y), xytext=(0,0),  textcoords='offset points')\nplt.show()","metadata":{"id":"CJlljN2gYTp5","outputId":"cdd792e6-0a15-4189-d611-02b1aa42ddb1","execution":{"iopub.status.busy":"2022-11-20T17:22:45.112829Z","iopub.execute_input":"2022-11-20T17:22:45.113455Z","iopub.status.idle":"2022-11-20T17:22:46.910452Z","shell.execute_reply.started":"2022-11-20T17:22:45.113383Z","shell.execute_reply":"2022-11-20T17:22:46.909262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n### ** >>> Exercise 8 (Take home): **  \n\nGenerate a t-SNE visualization to show the 15 words most related to the words \"angry\", \"happy\", \"sad\", \"fear\" (60 words total).","metadata":{"id":"2PL61rqYYTp5"}},{"cell_type":"code","source":"# Answer here\nword_list = ['angry','happy','sad','fear']\n\ntopn = 14\nangry_words = ['angry'] + [word_ for word_, sim_ in word2vec_model.wv.most_similar('angry', topn=topn)]\nhappy_words = ['happy'] + [word_ for word_, sim_ in word2vec_model.wv.most_similar('happy', topn=topn)]        \nsad_words = ['sad'] + [word_ for word_, sim_ in word2vec_model.wv.most_similar('sad', topn=topn)]        \nfear_words = ['fear'] + [word_ for word_, sim_ in word2vec_model.wv.most_similar('fear', topn=topn)]        \n\nprint('angry_words: ', angry_words)\nprint('happy_words: ', happy_words)\nprint('sad_words: ', sad_words)\nprint('fear_words: ', fear_words)\n\ntarget_words = angry_words + happy_words + sad_words + fear_words\nprint('\\ntarget words: ')\nprint(target_words)\n\nprint('\\ncolor list:')\ncn = topn + 1\ncolor = ['b'] * cn + ['g'] * cn + ['r'] * cn + ['y'] * cn\nprint(color)","metadata":{"id":"rvh7ymeNYTp5","execution":{"iopub.status.busy":"2022-11-20T17:22:46.912132Z","iopub.execute_input":"2022-11-20T17:22:46.912461Z","iopub.status.idle":"2022-11-20T17:22:46.930842Z","shell.execute_reply.started":"2022-11-20T17:22:46.912431Z","shell.execute_reply":"2022-11-20T17:22:46.928952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## w2v model\nmodel = word2vec_model.wv\n\n## prepare training word vectors\nsize = 200\ntarget_size = len(target_words)\nall_word = list(model.index_to_key)\nword_train = target_words + all_word[:size]\nX_train = model[word_train]\n\n## t-SNE model\ntsne = TSNE(n_components=2, metric='cosine', random_state=28)\n\n## training\nX_tsne = tsne.fit_transform(X_train)\n\n## plot the result\nplt.figure(figsize=(7.5, 7.5), dpi=115)\nplt.scatter(X_tsne[:target_size, 0], X_tsne[:target_size, 1], c=color)\nfor label, x, y in zip(target_words, X_tsne[:target_size, 0], X_tsne[:target_size, 1]):\n    plt.annotate(label, xy=(x,y), xytext=(0,0),  textcoords='offset points')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-11-20T17:22:46.934332Z","iopub.execute_input":"2022-11-20T17:22:46.935300Z","iopub.status.idle":"2022-11-20T17:22:48.823452Z","shell.execute_reply.started":"2022-11-20T17:22:46.935232Z","shell.execute_reply":"2022-11-20T17:22:48.822271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{"id":"_fF1woa8YTp5"}},{"cell_type":"markdown","source":"# COMPETITION REPORT","metadata":{}},{"cell_type":"markdown","source":"# 1. Data Preparation","metadata":{}},{"cell_type":"markdown","source":"## 1.1 Load Data\nread and load the dataset ","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n###training data\ndata_train = pd.read_csv(\"../input/kaggle-competition/data_identification.csv/data_identification.csv\",\n                         sep=\",\", header=None,names=[\"tweet_id\", \"identification\"])\nemotion_train = pd.read_csv(\"../input/kaggle-competition/emotion.csv/emotion.csv\",\n                            sep=\",\", header=None, names=[\"tweet_id\", \"emotion\"])\ntweets_train = pd.read_json(\"../input/kaggle-competition/tweets_DM.json/tweets_DM.json\",lines=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# re-format the json file\ntweets_train = tweets_train.drop(['_source'],axis=1).merge(pd.json_normalize(tweets_train['_source']),left_index=True,right_index=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# combine 4 sub-dataset for training data\ntrain_df = tweets_train.merge(data_train[data_train.identification.eq('train')],left_on='tweet.tweet_id',right_on='tweet_id').merge(emotion_train,left_on='tweet.tweet_id',right_on='tweet_id').drop(['tweet_id_x','tweet_id_y'],axis=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# combine 4 sub-dataset for testing data\ntest_df = tweets_train.merge(data_train[data_train.identification.eq('test')],left_on='tweet.tweet_id',right_on='tweet_id').drop(['tweet_id'],axis=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.2 Exploratory data analysis (EDA)\nto understand some insight of the data","metadata":{}},{"cell_type":"code","source":"train_df.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Shape of Training df: \", train_df.shape)\nprint(\"Shape of Testing df: \", test_df.shape)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#group to find distribution\ntrain_df.groupby(['emotion']).count()['tweet.text']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.3 Save data\n\nmore convenient for cross-platform use, use less space to store also consume less loading time","metadata":{}},{"cell_type":"code","source":"#save to pickle file\ntrain_df.to_pickle(\"train_df.pkl\") \ntest_df.to_pickle(\"test_df.pkl\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\n#load a pickle file\ntrain_df = pd.read_pickle(\"train_df.pkl\")\ntest_df = pd.read_pickle(\"test_df.pkl\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2.  Feature Engineering\n**Using Bag of Words**\n\nCountVectorizer perform word frequency and use these as features to train a model. We convert variable-length texts into a fixed-length vector so that it will be a structured, well defined fixed-length input.\n\nPerform lemmatization as normalization technique to improve text matching. Lemmatization normalizes a word with the context of vocabulary and morphological analysis of words in text. This algorithm analyzes the structure of the word and its context to convert it to a normalized form.","metadata":{}},{"cell_type":"code","source":"import nltk\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom nltk.stem import WordNetLemmatizer\n\n#perform lemmatization\nlemmatizer = WordNetLemmatizer()\n\n#Initialize a CountVectorizer object\nBOW_vectorizer = CountVectorizer(tokenizer=nltk.word_tokenize)\n\n#Transform the training data using only the 'text' column values\nX_train1 = BOW_vectorizer.fit_transform(train_df['tweet.text'])\ny_train1 = train_df['emotion']\n\n#Transform documents to document-term matrix\nX_test1 = BOW_vectorizer.transform(test_df['tweet.text'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Result Evaluation\nCheck the model's performance. Build a model using a Multinomial Naive Bayes model and train it. This model provides an ability to classify textual data with significantly reduced complexity.","metadata":{}},{"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\n\n#Instantiate a Multinomial Naive Bayes classifier\nNB_model = MultinomialNB()\n\n#Fit the classifier to the training data\nNB_model = NB_model.fit(X_train1, y_train1)\n\n#Create the predicted tags\ny_test_pred_NB = NB_model.predict(X_test1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(y_test_pred_NB).head(10)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#combine original and predicted dataset\nfinal1 = test_df.merge(pd.DataFrame(y_test_pred_NB),left_index=True,right_index=True).sort_index()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#function to remove unnecessary columns\ndef preexport (x):\n    return x.drop(['_score','_index','_crawldate','_type','tweet.hashtags','tweet.text','identification'],axis=1).rename(columns={'tweet.tweet_id':'id',0:'emotion'})","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final1 = preexport(final1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#save to csv file\nfinal1.to_csv(\"./final1.csv\",index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"At first, I tried implementing TfidfVectorizer with Multinomial Naive Bayes. Then, I found out that the combination of CountVectorizer with Multinomial Naive Bayes gives a higher accuracy score compared to TfidfVectorizer with Multinomial Naive Bayes. This is also the case in \"A SCALABLE SHALLOW LEARNING APPROACH FOR TAGGING ARABIC NEWS ARTICLES\". (https://www.researchgate.net/publication/343313467_A_SCALABLE_SHALLOW_LEARNING_APPROACH_FOR_TAGGING_ARABIC_NEWS_ARTICLES)\nEven though TfidfVectorizer is supposedly better than CountVectorizer, it still depends on the dataset and use of classifier.","metadata":{}}]}